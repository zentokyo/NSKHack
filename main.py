import sys
import os
from pathlib import Path

sys.path.append(str(Path(__file__).parent))

from core.search import HybridSearchAgent
from utils.file_processor import FileProcessor
from core.embedding import EmbeddingModel
from core.database import VectorDatabase
import argparse
import time


def initialize_database():
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")

    processor = FileProcessor()
    embedding_model = EmbeddingModel()
    vector_db = VectorDatabase(embedding_model)  # ‚úÖ –ø–µ—Ä–µ–¥–∞—ë–º –º–æ–¥–µ–ª—å

    print("–û–±—Ä–∞–±–æ—Ç–∫–∞ Markdown —Ñ–∞–π–ª–æ–≤...")
    chunks, metadatas = processor.process_markdown_files("data/knowledge_base")

    if not chunks:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ .md —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    print("–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    batch_size = 3

    all_embeddings = []
    valid_chunks = []
    valid_metadatas = []

    for i in range(0, len(chunks), batch_size):
        batch_chunks = chunks[i:i + batch_size]
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i // batch_size + 1}/{(len(chunks) - 1) // batch_size + 1}")

        batch_embeddings = embedding_model.get_embeddings_batch(batch_chunks)

        for j, (chunk, embedding, metadata) in enumerate(
                zip(batch_chunks, batch_embeddings, metadatas[i:i + batch_size])):
            if embedding is not None and len(embedding) == 768:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
                all_embeddings.append(embedding)
                valid_chunks.append(chunk)
                valid_metadatas.append(metadata)
                print(f"–ß–∞–Ω–∫ {i + j} - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {len(embedding)}")
            else:
                print(f"–ß–∞–Ω–∫ {i + j} - –ø—Ä–æ–ø—É—â–µ–Ω (None –∏–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å)")

        time.sleep(0.5)

    print(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(valid_chunks)} –∏–∑ {len(chunks)} —á–∞–Ω–∫–æ–≤")

    if not valid_chunks:
        print("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –±–∞–∑—É")
        return

    print("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É...")

    ids = [f"chunk_{i}" for i in range(len(valid_chunks))]

    vector_db.add_documents(
        documents=valid_chunks,
        metadatas=valid_metadatas,
        ids=ids,
        embeddings=all_embeddings
    )

    print("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")


def interactive_mode():
    agent = HybridSearchAgent()
    print("=" * 60)
    print("–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∞–≥–µ–Ω—Ç —Å llama3.2 –∑–∞–ø—É—â–µ–Ω")
    print("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
    print("=" * 60)

    while True:
        try:
            query = input("\nüß† –í–æ–ø—Ä–æ—Å: ").strip()
            if query.lower() in ['quit', 'exit', 'q']:
                break

            if not query:
                continue

            print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞...")
            response = agent.process_query(query)
            print(f"\n–û—Ç–≤–µ—Ç: {response}")

        except KeyboardInterrupt:
            print("\n–í—ã—Ö–æ–¥...")
            break
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AI Agent —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –∏ llama3.2")
    parser.add_argument("--init", action="store_true", help="–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
    parser.add_argument("--chat", action="store_true", help="–ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º")

    args = parser.parse_args()

    if args.init:
        initialize_database()
    elif args.chat:
        interactive_mode()
    else:
        print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ:")
        print("  --init   –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã")
        print("  --chat   –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞")
